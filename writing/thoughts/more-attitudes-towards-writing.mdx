---
title: More attitudes towards writing
description: Further thoughts on writing with AI
published: 2023/12/31
duration: 18 min
tags: ['writing', 'ai']
---

Today's large language models (LLMs) can provide a fast, low effort and usefully unpredictable way of generating entirely mediocre content.

An LLM, like any other tool, offers us a way of making our desired outcome a reality. They are production machines with easily apparent strengths and weaknesses - professionals in the field have found that LLMs are particularly useful for beating writers block and are decent summarizers, but generally terrible at new conceptual thought.

LLMs allow people who are busy and tired to create written works with next to no effort, offering blocked creative visions a new path to reality. Existing creators benefit in a similar way - there's much less effort required to make high fidelity prototypes, so professionals have more opportunity for play and experimentation. Instead of working on a book over months, a first draft can be made over the course of a week. Of course, there is an easier solution than using LLMs for those facing issues of limited time for creative endeavor - own more capital.

Unfortunately, beyond the first draft the quality of our current model's output will not be better than a human's work, unless produced by an experienced prompter and edited by a professional. If you're looking for something better than passable output, you'll need more money and time. The highest quality models are proprietary, requiring payment for unlimited access, while 'free' open source models run locally require expensive GPU computer setups, generally producing lower quality results unless fine-tuned towards your goal. Taking advantage of the latest open source resources can take a lot of technical knowledge and time to perfect.

Fortunately, these are early 2023 problems which will gradually be eroded through community contributions. When we reach the end goal of near effortless, free creation of high quality writing, we'll probably start asking different questions: what do we actually want to write? Who are we writing for - ourselves, our friends, the next LLM iteration? Should we be pushing ourselves to write more and faster in our already hectic environment?

#### A rather hectic environment

"There's an ongoing trend pushing towards continuous consumption of shorter, mind-melting content. Have a few minutes? Stare at people putting on makeup on TikTok. Winding down for sleep? A perfect time to doomscroll 180-character hot takes on Twitter. Most of the products I've seen built with LLMs push us further down this road: why write words when an AI can write that article for you? Why think when AI can write your code?

When I try these new products, I find myself transported into WALL-E. My brain turns off and I press the magic ðŸª„ button or mash the Tab key. And when I'm eventually jolted out of my zombie mode, I don't even really like what's been created."

https://wattenberger.com/thoughts/boo-chatbots

Should we even be writing if we are tired and exhausted? Sacrificing personal health and wellbeing in the name of a writing project is dire. Maybe it's necessary or you think it's worth paying the price for - at this point I'd caution you that working quickly likely leads to more mistakes.

What matters more than speed is taking the time to create the right thing.

Will we find catharsis in journalling if AI presupposes or interprets how we are feeling?

### Discover new insights

Linus from theSephist discusses a mental model of writing where we resolve a maze of associated ideas towards achieving a goal. He quotes Henrik, who describes writing as choosing an interesting starting thought and following it though a chain of connections. These approaches feel surprisingly narrow minded to me; writing and thought are disjointed over several sessions, often discussed with others, and maybe I feel just a bit peeved at having the complex near impossibility of the mind reduced to an entirely logical graph traversal - our emotions and circumstances have an impact to.

If we're trying to make a path through a maze of ideas, Linus suggests that it would be helpful if we were aware of the most relevant ideas relayed and connections explored by others, surrounding our starting point. In his context, valuable or worthwhile writing follows most novel and interesting paths of ideas - implying that an idea that we've seen before may not be so valuable, as it's already been written. AI tools can be particularly useful for research - it's annoying to miss out on what other people have said about a topic, and perhaps makes the writing not as useful to our readers.

In the case of two plus two equalling four, I admit I will be a bit exasperated the fifth time you attempt to explain the concept to me. However, some ideas aren't so commonly known, or haven't been as recently explained - why not amplify them? What if our idea is not online, so our large language models don't have access to it? Countless years of oral tradition and the incredible diversity of key stakeholder's unspoken opinions might as well not exist (at least until the AI also learns to carry out thoughtful interviews on our behalf).

When we are presented with a map of what already exists, we also lose some of our own fresh perspective; the more we read, the more we're filled in with information that describes the situation from established perspectives, prone to the bias and knowledge deemed most relevant to share by the machine. What is worse, we might find the generally accepted answer to the question we're exploring - and prompted with a passible solution, our typically laxidasical minds might not make much of an effort to reach past it and consider the space of much better solutions and opportunities. Effective use of AI tools requires a rapid increase in our critical thinking skills, as experienced by anyone who has asked Bing's AI search tool a non-trivial series of questions. If AI can find a connection between two ideas and we're satisfied with that, we may stop investigating a chain of thought that, several weeks of thought later, could form something much greater and more useful.

It's not terrible to rediscover an existing idea manually, unless considered in a purely capitalistic context of ideas owned by individuals for status and patents. Completing a thesis and realizing you've uncovered an existing idea, you can compare your reasoning and explore how it relates. I also wonder if there is some value in many independent people arriving at the same conclusion through their own reasoning, an extension of the scientific method. I also wonder if more people use the same tools, will we see an increase in people arriving at the same answers and attempting to make the most of the same opportunities? This is somewhat unlikely unless everyone is asking the same questions.

This also reminds me of Borges's story of a writer who set out to write a book already written by another author, using only knowledge of the author and the context they lived in to reconstruct the author's words exactly as he wrote them. The output is worthless, but the journey of perspective and empathy to get there, the depth of understanding of the source material, incredible.

It might also not be bad to forget an older idea that just isn't as useful anymore for your wider aims. Eventually AI will have enough intelligence to jettison and streamline existing ideas for us, relevant to our goals, but until then we might be trapped in a hut of old ideas and a lot of noise: the more we write quickly with AI, the more there is to acknowledge in the pre-existing maze of ideas. So we'll be living in a haze of noise until it's cleared by the AI - or we could read less, take our time more, think for ourselves carefully for what matters to us, and explore that in a way that is meaningful to us.

<details>
  <summary>Aside On AI Concept Generation</summary>I do wonder if we'll enter an
  age of AI generated concepts which are more prevalent than human ones; when
  the machines produce more conceptual structures / ideas than humans, and we're
  left several miles behind, wondering at the intricacy and complexity of their
  reasoning, the foreign nature of their words, the seeming adjacency of the
  reality they understand and perceive. Of course, the AI could teach us to see
  and understand their world, but we wouldn't necessarily be anywhere near the
  forefront of their conceptual creation - our understanding would be the
  uncovering of already existing ideas, explored for the joy of learning.
  Importantly, in this context, it is the individual's understanding and their
  experience that matters more than the collective. The concepts that AI
  generates, just like the concepts of today, will shape how we perceive and
  interact with the world in our own lives. With enough conviction and the means
  to share and justify concepts, any human's mode of thought could be
  transformed into perceiving reality in the means largely defined by machines.
  We already see similar warping of reality in cults and radical online groups
  such as QAnon.
</details>

Even if the idea is common knowledge, a fresh explanation will still be somewhat novel. When formed, ideas exist, fade and resurface constantly in future years; maybe you've heard of this idea before, in Frans Masreel's woodcut animated film 'The Idea', or a certain author's manuscript not burning, or Borges lengthy footnote tracing the core idea of the universe and our place in it across hundreds of years in various texts. Borges literary flexing is somewhat interesting and useful for the researcher, but probably not always necessary.

Worrying about the novelty of our ideas, how we relay them, whether they are useful to others, might get in the way of something important: having fun while writing. I think we need a balance between the capitalized, optimized, super efficient academically complete brand new TM certified idea, (complete with citations and references) and the joy of exploring an unknown path and forming conclusions in a rather messy and human way, getting our own brain in order.

It's weird describing ideas in the abstract like this - surely I should write with actual examples, instead of theorising!

There are two types of writing here: one that pushes a collective frontier forwards, another that extends a personal frontier. Some excellent writing combines both.

LLMs connected to search engines give useful and detailed overviews of fields of existing knowledge, streamlining the early phases of research where we're uncertain of the right questions to ask. An LLM can summarize pages of search results, hopping over the SEO optimized, pay per click sponsored sites to pick what it deems appropriate notes from a five minute article. I've experienced slight annoyance recently whenever I manually search for something and none of the first page search results are relevant - it feels like a natural conversational response is better, simply because I don't have to navigate onto a new webpage.

However, there is value in taking the time to grapple with a complex topic yourself, to manually move the ideas around in your mind until they make sense. The more time stewing on a subject, more related ideas arise, the more opportunities to note adjacent ideas. An interesting piece of writing generally takes us somewhere new.

If we're not careful, we'll outsource our understanding to the AI, and also our idea generation. We are way too soon to do that judging by the poor quality of Bing chat's understanding of the world - only citing results from the top ten articles creates relatively unhelpful and occasionally distracting answers to my questions. For research purposes, a much larger trawl of information is required and potentially even the initiative to ask reflective questions on what it finds, then rewriting it's response to include that information. Bing chat's first weakness is searching the users question almost verbatim, when it should ideally know to search a smarter question that includes more information - what the author actually wants to know.

The more information we have for writing, the easier the access to that information, the greater the temptation to include that information even if it's not salient. As our early stage searching chatbots demonstrate, they take top search results, then process them into a streamlined output - when using AI tools, do we risk also mimicking their form? Blending of the medium and message seems particularly fickle in the world of words.
